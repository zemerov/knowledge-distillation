# Survey on Knowledge Distillation for sequence binary classification 

## Motivation 
In the original [paper]https://arxiv.org/abs/1503.02531 the knowledge distillation was applied for multiclass classification (MNIST - 10 classes, speech recognition 14000, JFT dataset 15000). 
The main question of this survey: Does the knowledge distillation approach from paper work on just 2 classes.

### Dataset 
IMDB 50k 
This dataset contains 50k comments with positive or negative labels. 

### Teacher model
